# 5 Dimensionality reduction techniques

```{r, echo=FALSE}
# *Describe the work you have done this week and summarize your learning.*
#
#- Describe your work and results clearly. 
#- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
#- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.
```

    Intro from the IODS MOOC-page:
    
Actually, a fairly large selection of statistical methods can be listed under the title "*dimensionality reduction techniques*". Most often (nearly always, that is!) the real-world phenomena are **multidimensional**: they may consist of not just two or three but 5 or 10 or 20 or 50 (or more) dimensions. Of course, we are living only in a three-dimensional (3D) world, so those multiple dimensions may really challenge our imagination. It would be easier **to reduce the number of dimensions** in one way or another.  

We shall now learn the basics of two data science based ways of reducing the dimensions. The principal method here is **principal component analysis** (PCA), which reduces any number of measured (continuous) and correlated variables into a few uncorrelated components that collect together as much variance as possible from the original variables. The most important components can be then used for various purposes, e.g., drawing scatterplots and other fancy graphs that would be quite impossible to achieve with the original variables and too many dimensions.  

**Multiple correspondence analysis** (MCA) and other variations of CA bring us similar possibilities in the world of **discrete variables**, even nominal scale (classified) variables, by finding a suitable transformation into continuous scales and then reducing the dimensions quite analogously with the PCA. The typical graphs show the original classes of the discrete variables on the same "map", making it possible to reveal **connections** (correspondences) between different things that would be quite impossible to see from the corresponding cross tables (too many numbers!).  

Briefly stated, these methods help to visualize and understand multidimensional phenomena by reducing their dimensionality that may first feel impossible to handle at all.

## 5.1 Introducing the dataset for PCA

>This chapter's dataset originates from the United Nations Development Programme. The Human Development Index (HDI) was created for assessig the development of a country in another way than just economic growth. More information can be found on their [general data page](http://hdr.undp.org/en/content/human-development-index-hdi) and on [pdf about calculating the human development indices](http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf). 

```{r}
# read the human data, row names as first column
human <- read.csv("./data/human.csv", row.names=1)
str(human)
```

The dataset consists 155 observations (i.e. **countries**) in 8 variables: 

1. Edu2.FM -- Ratio of secondary education in females compared to males (numeric: from 0.1717 to 1.4967)
2. Labo.FM -- Ratio of labour force participation rate in females compared to males (numeric: from 0.1857 to 1.0380)
3. Life.Exp -- Life expectancy at birth (numeric: from 49 to 83.5)
4. Edu.Exp -- Expected years of schooling (numeric: from 5.4 to 20.2)
5. GNI -- Gross national income per capita (numeric: from 581 to 123124)
6. Mat.Mor-- Maternal mortality ratio (numeric: from 1 to 1100)
7. Ado.Birth -- Adolescent birth rate (numeric: 0.6 to 71.95)
8. Parli.F -- Percentage of female representatives in parliament (numeric: from 0 to 57.5)

The data combines several indicators for the countries:

* **Country**: name of the country as row name
* **Health and knowledge**: `GNI`, `Life.Exp`, `Edu.Exp`, `Mat.Mor`, `Ado.Birth`
* **Empowerment**: `Parli.F`, `Edu2.FM`,`Labo.FM`

Most of the variable names have been shortened from the original data and two new variables (`Edu2.FM` and `Labo.FM`) were computed.

## 5.2 Graphical overview and summaries of the variables

```{r message=FALSE}
library(ggplot2) # for graphics
library(GGally)
library(corrplot)
library(dplyr)
summary(human)
ggpairs(human, upper = list(continuous = wrap("cor", family="sans"))) # graphical overview
```

There is much **skewness** present in the variable, i.e. only `Edu2.FM` and `Edu.Exp` are somewhat normally distributed. Many variables are strongly correlated as implied by the statistically significant correlation coefficients. The correlation can be visualized better with correlation plot:


```{r message=FALSE}
# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot(type="lower") # lower correlation matrix, since symmetric
```

The correlation matrix gives us an idea of the relation of the variables, i.e. there seem to be great  

* **negative correlations** between maternal mortality ratio and life expectency and expected years of schooling and ratio of secondary education in females/males (`Mat.Mor` and `Life.Exp`&`Edu.Exp`&`Edu2.FM`). Similarly with Adolescent birth rate `Ado.Birth`.  

* **positive correlations** between Education expectency and Life expectency and Adolescent birth rate and Maternal mortality

Percentage of female representatives in parliament (`Parli.F`) and Ratio of labour force participation rate in females compared to males (`Labo.FM`) don't seem to be correlated with any variable. There is, however, a slight correlation between them.


## 5.3 Principal Component Analysis 

**Principal component analysis** (PCA) is a statistical procedure which reduces the number of dimensions in multivariate data. Reducing dimension is essential in representing the phenomenon of interest clearly without "too much" distracting information (non-related data, noise or random error).  

The idea of PCA is to *transform* the data to a new space with equal or less number of dimensions (new features). These new features are called **principal components** (PC). The first PC captures the **maximum amount of variance** from the features in the original data. The second PC captures the **maximum amount of variability left** etc. All PC are **uncorrelated** and **orthogonal** to each other.  

Since **PCA is sensitive to variance**, let us make two analyses: one without and one with standardized dataset.

### 5.3.1 PCA without standardization of data 

```{r message=FALSE, warning=FALSE}
# PCA on non-standardized human data (with the SVD method)
pca_human <- prcomp(human)
# printing summary of PCA
s <- summary(pca_human)
s    
# print out rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot of the PC representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8,1), col=c("grey40","deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2], caption="PC1 is the strongest")
```

### 5.3.2 PCA with standardization of data

```{r message=FALSE, warning=FALSE}
# standardize the variables by the function scale (the z-score)
human_std <- scale(human)
# print out summaries of the standardized variables (mean = 0)
summary(human_std)

# perform principal component analysis (with the SVD method)
pca2_human <- prcomp(human_std)
# printing summary of PCA
s2 <- summary(pca2_human)
s2    
# print out rounded percetanges of variance captured by each PC
pca2_pr <- round(100*s2$importance[2, ], digits = 1)
pca2_pr
# create object pc_lab to be used as axis labels
pc_lab2 <- paste0(names(pca2_pr), " (", pca2_pr, "%)")
# draw a biplot of the PC representation and the original variables
biplot(pca2_human, choices = 1:2, cex=c(0.8,1), col=c("grey40","deeppink2"), xlab = pc_lab2[1], ylab = pc_lab2[2], caption="After standardization of variables")
```

### 5.3.3 Interpreting the results

The results are *very different* based on whether or not the standardization was made.  

Without normalization of data, there is practically only one PC which explains 100% of the variance in the data. The variable gross national income per capita (`GNI`) dominates the PCA due to its absolute values being 10- or even 100-fold larger than the other variables. Thus, as expected, **there is a crucial need for standardization**!  

After standardization, the effect of other variables than just `GNI` becomes clearer. The first PC explains 53.6% of the variability and the second 16.2% covering roughly a total of 70% of the variability in the data. Out of the variables, `Parli.F` and `Labo.FM` have high positive correlation with PC2 (the angle between the y-axis is small) while all other variables have a high correlation with PC1 (x-axis).  This kind of division could be observed already in the correlation plot earlier.

Regarding the interpretation of the dimensions in the standardized data PCA, it seems that PC1 is some general index for health and knowledge ("standard of living") while PC2 is some female empowerment in the society.

## 5.4 Multiple Correspondence Analysis

While *PCA* reduced the dimension for *continuous* variables, **Multiple Correspondence Analysis** (MCA) is an analogous method for **categorical** variables. Let's apply the MCA to `tea` dataset from the package `FactoMineR`. The tea dataset consists of tea consumption survey results from 300 tea consumers. For our purpose we will choose only six columns.

```{r message=FALSE}
library(tidyr)
library(FactoMineR)
data("tea") # Load data tea from FactoMineR package
# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
# select the 'keep_columns' to create a new dataset
tea_time <- dplyr::select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
str(tea_time)
summary(tea_time)
```

Exploring our data reveals that we have 300 observations in 6 variables. All the variables are of type `factor` with several levels. Let's visualize the dataset.

```{r message=FALSE}
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

All of the variables have a certain dominating factor except for `sugar` which evenly divided between *sugar* and *no sugar*.  

Now let's perform a MCA to our `tea` dataset!

```{r message=FALSE}
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
```

The output of the MCA summary is somewhat lengthy. However, it contains information about  

* **Eigenvalues**: the variances and the percentage of variances retained by each dimension

* **Individuals**: the individuals' coordinates, the individuals contribution (%) on the dimension and the cos2 (the squared correlations) on the dimensions

* **Categories**: the coordinates of the variable categories, the contribution(%) the cos2 (the squared correlations) and v.test value. The v.test follows normal distribution: if the value is below/above +/- 1.96, the coordinate is significantly different from zero.

* **Categorical variables**: the squared correlation between each variable and the dimensions. If the value is close to 1 it indicates strong link with the variable and dimension.  

Here it seems there is no dominating dimensions (the first and the second dimension explain only about 15% and 14% of the variation, respectively).

```{r message=FALSE}
# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```

Visualizing the data does not give a better insight about the matter either. By interpreting the biggest ranges in the dimentions, the first dimension might tell about how the tea is consumed (in a tea bag vs. unpackaged) while the second dimension tells where the tea is bought (tea shop vs. chain store). Observing the clusters, it seems that e.g. ```unpackaged``` and ```tea shop``` are very similar as well as ```tea bag``` and ```chain store``` tea have a connection. However, many of the connections remain fuzzy. It is sometimes possible that despite all the advanced methods, obtaining insights from the data is difficult. 
