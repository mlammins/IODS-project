# 4 Clustering and classification

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

## 4.1 Introducing the dataset

>This chapter's dataset consists of housing values in suburbs of Boston (the [Boston data](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) from the MASS package). 

```{r}
# access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
```

The dataset contains 506 observations in 14 variables: 


1. crim -- per capita crime rate by town (numeric: from 0.00632 to 88.9762)
2. zn -- proportion of residential land zoned for lots over 25,000 sq.ft. (numeric: from 0 to 100)
3. indus -- proportion of non-retail business acres per town (numeric: from 0.46 to 27.74)
4. chas -- Charles River dummy variable (binary: 1 if tract bounds river, 0 otherwise)
5. nox -- nitrogen oxides concentration (parts per 10 million) (numeric: from 0.385 to 0.871)
6. rm -- average number of rooms per dwelling (numeric: from 3.561 to 8.78)
7. age -- proportion of owner-occupied units built prior to 1940 (numeric: 2.9 to 100)
8. dis -- weighted mean of distances to five Boston employment centres (numeric: from 1.13 to 12.127)
9. rad -- index of accessibility to radial highways (numeric: from 1 to 24)
10. tax -- full-value property-tax rate per \$10,000 (numeric: from 187 to 711)
11. ptratio -- pupil-teacher ratio by town (numeric: from 12.6 to 22)
12. black -- 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town (numeric: from 0.32 to 396.9)
13. lstat -- lower status of the population (percent) (numeric: from 1.73 to 37.97)
14. medv -- median value of owner-occupied homes in \$1000s (numeric: from 5 to 50)

## 4.2 Graphical overview and summary of the data

The function pairs gives a rough visual idea of the data while summary describes the variables numerically.

```{r message=FALSE}
library(dplyr)
library(corrplot)
pairs(Boston) # done as in Data Camp, but it is so small, almost impossible to see anything
summary(Boston)
```

Based on the numerical summary, it seems variables `crim`, `zn`, `indus`,`dis`, `rad` have quite low values while `rm`, `age` and `black` seem to have higher values. The output of pairs is very difficult to read, so let's try calculating the correlation matrix to see the relationships between the variables. The (rather large) correlation matrix is easier to interpret when visualized with corrplot-function.
```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 

# print the correlation matrix, kable for nicer looking table
knitr::kable(
cor_matrix %>% round(digits=2)
)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```

From the correlation matrix we can see that there is a strong

* negative correlation between distance and proportion of units built before 1940, nitrogen oxide concentration and proportion of non-retail business acres (`dis`&`age`, `dis`&`nox`, `dis`&`indus`) as well as median value of home and lower status of the population (`medv`&`lstat`).

* positive correlation between property tax-rate and accessibility to radial high ways (`tax`&`rad`) among others.


## 4.3 Standardizing and splitting data into train and test sets
    
    
Standardization of the data is useful when the data has large differences between their ranges or when the data is measured in different measurement units. Let's scale the `Boston` data by subtracting the column means from the corresponding columns and divide the difference with standard deviation:
$$scaled(x) = \frac{x-mean(x)}{sd(x)}.$$
This is one of the most popular way of standardizing data, the **Z-score**. Now all variables have a mean of zero and a standard deviation of one. Thus they are on the same scale.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
```

Next we shall create a categorical variable `crime` (per capita crime rate by town) from the standardized data set. Let's cut the variable by quantiles to get the high, low and middle rates of crime into their own categories. Finally, let's drop the old crime rate variable from the data set.
    
```{r}
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))
# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

In order to test the predictive power of a statistical method, let's divide the scaled Boston data set randomly into a training set (80 %) and a test set (20 %).

```{r}
# number of rows in the Boston dataset 
n <- nrow(Boston)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
```

## 4.4 Fitting the LDA

The standardization was done to satisfy the conditions for using the **linear discriminant analysis** (LDA):

* variables are **normally distributed** (on condition of the classes)

* each variable has **the same variance**.

The general idea is to reduce the dimensions by removing redundant and dependent features by transforming the features from higher dimensional space to a space with lower dimensions. 


Now, let's fit LDA on the train set with the newly-created `crime` as the target variable and all other variables as predictor variables. The result can be visualised by the LDA (bi)plot.  
    
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

Here we can see the results of the LDA. Each color represents a class of the target variable. The predictor variables are the arrows in the middle of the picture, the length and the direction of the arrow depicting the effect of the predictor. It seems that here the variables `rad`, `zn` and `nox` discriminate/separate the classes the best. 

## 4.5 Predicting with the LDA

Now we use the built LDA model to predict the classes on the test data. LDA calculates the probability of the new observation for belonging in each of the classes and then the observation is classified to the class of the highest probability. 

First, let's save the correct classes and then remove the `crime` variable.

```{r}
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The prediction  would have been perfect if all the values were on the diagonal. Certainly this is not the case but the largest values are on the diagonal. There seems to be some mixing with the first three classes but the last class (`high`) is most correctly predicted. This was to be expected based on the training set figure.


## 4.6 Distances and clustering
    
Different distances (e.g. Euclidean or Manhattan) are used to see if observations are similar or dissimilar with each other. Similar observations form clusters which can be found by different methods (e.g. k-means).

Let's find clusters on the `Boston` dataset using k-means. First, let's reload the dataset and standardize it to get comparable distances (Euclidean and Manhattan). Then let's run the k-means algorithm on the dataset.

```{r message=FALSE}
# reload Boston from MASS
library(MASS)
library(ggplot2)
data('Boston');
# center and standardize variables
boston_scaled <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)
# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled,method="manhattan")
# look at the summary of the distances
summary(dist_man)
```

On can see that the Manhattan distance gives much larger values compared to the Euclidean distnace. For now, however, let's use the Euclidean distance.  

```{r}
# k-means clustering
km1 <-kmeans(boston_scaled, centers = 1)
km2 <-kmeans(boston_scaled, centers = 2)
km3 <-kmeans(boston_scaled, centers = 3)
km4 <-kmeans(boston_scaled, centers = 4)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km1$cluster) # 1 cluster
pairs(boston_scaled, col = km2$cluster) # 2 clusters
pairs(boston_scaled, col = km3$cluster) # 3 clusters
pairs(boston_scaled, col = km4$cluster) # 4 clusters
# too general view, make smaller
pairs(boston_scaled[,6:10], col = km2$cluster) # 2 clusters
pairs(boston_scaled[,6:10], col = km3$cluster) # 3 clusters
pairs(boston_scaled[,6:10], col = km4$cluster) # 4 clusters
```

Different number of centers (1,2,3 or 4) were used for k-means clustering. One cluster seemed to be too few, since new clusters started appearing, however, four clusters did not bring a dramatic difference to the game  (i.e. the centroid and the clusters did not change). Thus the optimal number was found to be 2 or 3 clusters.

```{r}
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')+scale_x_continuous(breaks = 1:10,labels=1:10)
```

The total within sum of squares (TWSS) would indicate 2 be the optimal number, since that is the number when TWSS changes radically (from 1 to 2).

## 4.7 3D LDA plot

```{r message=FALSE}
# Run the code below for the (scaled) train data that you used to fit the LDA. 
#The code creates a matrix product, which is a projection of the data points.
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Next, install and access the plotly package. Create a 3D plot (**Cool!**)
# of the columns of the matrix product by typing the code below."

library(plotly)
# Note! To install plotly in Linux, remember to install libcurl from terminal.
# * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)
# * rpm: libcurl-devel (Fedora, CentOS, RHEL)
# * csw: libcurl_dev (Solaris)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train$crime)
```
