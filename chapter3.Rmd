# 3 Logistic regression

```{r, echo=FALSE}
# *Describe the work you have done this week and summarize your learning.*

#- Describe your work and results clearly. 
#- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
#- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.
```

    Intro from the IODS MOOC-page:

One way to move on from linear regression is to consider settings where the dependent (target) variable is **discrete**. This opens a wide range of possibilities for modelling phenomena beyond the assumptions of continuity or normality.  

**Logistic regression** is a powerful method that is well suited for predicting and classifying data by working with **probabilities**. It belongs to a large family of statistical models called **Generalized Linear Models** (GLM). An important special case that involves a **binary** target (taking only the values 0 or 1) is the most typical and popular form of logistic regression.

We will learn the concept of **odds ratio** (OR), which helps to understand and interpret the estimated coefficients of a logistic regression model. We also take a brief look at **cross-validation**, an important principle and technique for assessing the performance of a statistical model with another data set, for example by splitting the data into a **training set** and a **testing set**.

## 3.1 Introducing the joined dataset

>The dataset describes student achievement in secondary education of two Portuguese schools. The dataset is joined from two datasets regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). The data attributes include student grades, demographic, social and school related features (and *especially alcohol consumption*) and it was collected by using school reports and questionnaires.
>
>More information: [https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance)


```{r}
alc <- read.csv("./data/alc.csv") # reading the analysis data
dim(alc) # number of rows and columns
str(alc) # type of data
```
The final **joined dataset** has 382 observations and 35 variables consisting of  only unique individuals. The datasets were joined by using the 13 student identifier variables: "school", "sex", "age", "address", "famsize", "Pstatus", "Medu", "Fedu", "Mjob", "Fjob", "reason", "nursery" and "internet". Only students present in both datasets were kept. The variables not used for joining the two data have been combined by averaging (including the grade variables). A more detailed information about the variables is presented below (possible values in parenthesis):

* 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
* 2 sex - student's sex (binary: 'F' - female or 'M' - male)
* 3 age - student's age (numeric: from 15 to 22)
* 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)
* 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
* 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
* 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 5th to 9th grade, 3 secondary education or 4 higher education)
* 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 5th to 9th grade, 3 secondary education or 4 higher education)
* 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
* 12 nursery - attended nursery school (binary: yes or no)
* 13 internet - Internet access at home (binary: yes or no)
* 14 guardian - student's guardian (nominal: 'mother', 'father' or 'other')
* 15 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
* 16 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
* 17 failures - number of past class failures (numeric: n if 1<=n<3, else 4)
* 18 schoolsup - extra educational support (binary: yes or no)
* 19 famsup - family educational support (binary: yes or no)
* 20 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* 21 activities - extra-curricular activities (binary: yes or no)
* 22 higher - wants to take higher education (binary: yes or no)
* 23 romantic - with a romantic relationship (binary: yes or no)
* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)
* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)
* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)
* 30 absences - number of school absences (numeric: from 0 to 93)

The grades G1, G2 and G3 are related to the course subject, Math or Portuguese. Variables alc_use and high_use were added to the original datasets:

* 31 G1 - first period grade (numeric: from 0 to 20)
* 32 G2 - second period grade (numeric: from 0 to 20)
* 33 G3 - final grade (numeric: from 0 to 20, output target)
* 34 alc_use - average of Dalc and Walc (numeric: from 1 - very low to 5 - very high)
* 35 high_use - high alcohol use status (binary: TRUE (if alc_use is higher than 2) or FALSE)


## 3.2 Purpose of the analysis and study hypothesis

The purpose of the analysis is to study the relationships between high/low alcohol consumption and some of the other variables in the data. Out of the many variables present,  **parent's cohabitation status**, **mother's education**, **quality of family relationships** and the **number of school absences** were chosen. Thus the study hypothesis are as follows:


* **Hypothesis 1**: If the parents are living apart (```Pstatus```='A'), the alcohol consumption (```high_use```) is higher
* **Hypothesis 2**: The higher the mother's education (```Medu```), the lower the alcohol consumption (```high_use```)
* **Hypothesis 3**: The higher the quality of family relationships (```famrel```), the lower the alcohol consumption (```high_use```)
* **Hypothesis 4**: The higher the number of school absences (```absences```), the higher the alcohol consumption (```high_use```)

**Note!** Now the target variable ```high_use``` is a binary variable (TRUE=1, FALSE=0) so we must use *logistic regression*.


## 3.3 Numerical and graphical exploration
Before making logistic regression, let's explore the distribution of chosen variables and their connection to the target variable numerically and graphically. First, let's take a look at **variable distributions**:
```{r message=FALSE}
library(dplyr);
# use dplyr to make a smaller dataset and include all chosen variables
alc_test <- select(alc, Pstatus, Medu, famrel, absences, alc_use, high_use)
# change Pstatus from char to factor
alc_test$Pstatus <- as.factor(alc_test$Pstatus)
# numerical summary of chosen variables
summary(alc_test[-c(5,6)])
# graphical exploration of variable distribution
par(mfrow = c(2,2))
#
barplot(table(alc_test$Pstatus), main="Distribution of Pstatus")
barplot(table(alc_test$Medu), main="Distribution of Medu")
barplot(table(alc_test$famrel), main="Distribution of famrel")
barplot(table(alc_test$absences), main="Distribution of absences")
```

From the data we can see that the vast majority of participants are living together with their parents (T). Also, most of the participants have good family relations (famrel=4) and the number of absences is relatively small (75 % less than 6). Mother's education is almost evenly distributed among variable values not counting zero. 

Out of curiosity, let's see how the **alcohol consumption** is distributed.
```{r}
summary(alc_test[c(5,6)])
par(mfrow = c(1,2))
barplot(table(alc_test$alc_use), main="Distribution of alc_use")
barplot(table(alc_test$high_use), main="Distribution of high_use")
```

It seems that only about one third of the participants use alcohol in high volumes. To better grasp the situation, here also the alcohol use (```alc_use```, numeric from 1 to 5) is shown instead of only the binary variable high use (```high_use```, TRUE if alc_use > 2).

Now let's see what is the relation of our chosen variables to alcohol consumption. Here also the numerical ```alc_use``` is used.

```{r}
par(mfrow = c(2,2))
boxplot(alc_use ~ Pstatus, data = alc)
boxplot(alc_use ~ Medu, data=alc)
boxplot(alc_use ~ famrel, data=alc)
boxplot(alc_use ~ absences, data=alc)
```

These boxplots give a rough idea of the relationships between variables and the target variable:

* **Pstatus vs. alc_use**: Roughly the same Q1 to Q3 area, but the median is smaller in A group. This is contrary to what was hypothesized.
* **Medu vs. alc_use**: Certainly group 0 has the greatest median for alc_use and the trend seems to go down as Medu increases, but group 3 and 4 do have big variability in values (Q1-Q3 cover a large area).
* **famrel vs. alc_use**: Nothing conclusive on the hypothesis. Excellent family relations (famrel=5) seems to protect from high alcohol consumption (smaller median, smaller Q1-Q3 area).
* **absences vs. alc_use**: Alcohol use seems to increase with absences which supports the hypothesis.


## 3.4 Logistic regression - the model

Time to form a mathematically rigorous model using logistic regression! Note that Pstatus will be addressed here as [factor](https://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression). To summarize:

**Target variable** = ```high_use``` 

**Chosen variables**: ```Pstatus```, ```Medu```, ```famrel```, ```absences```

```{r message=FALSE}
m <- glm(high_use ~ Pstatus+Medu+famrel+absences, data = alc_test, family ="binomial")
# print out a summary of the model
summary(m)
```

The most important insight from the model summary is the coefficient section. Using the estimated coefficients, **the obtained model** is

> high_use $= -0.45+0.17 \times PstatusT-0.01 \times Medu-0.24 \times famrel+0.09 \times absences$

Since the target valuable ```high_use``` is binary, it has only values 0 (as FALSE or "failure") and 1 (as TRUE or "success) in modelling sense. The coefficients of the fitted model can be thus interpreted as

* **PstatusP**: if Pstatus=A, then PstatusT=0 and the coefficient is -0.45 meaning parents living apart actually lessens the probability of high alcohol use. If Pstatus=T, then the effect is -0.45+0.17=-0.28 which is still negative but not as much as with Pstatus=A.
* **Medu**: higher level of mother's education lessens the probability for high alcohol use but the effect is quite negligible.
* **famrel**: negative coefficient means high value for famrel lessens the probability of high_use.
* **absences**: positive value for coefficient means that increased number of absences increases the probability of high_use

Of the variables, only famrel and absences were found to be statistically significant on 0.05 and 0.001 levels, respectively.  

***

The model coefficients can also be interpreted as odds ratio.

>**Odds**: the ratio of expected "successes" to "failures", i.e. $\frac{p}{1-p}$ with value ranging from 0 to infinity.   

So higher odds correspond to a higher probability of success. They are an alternative way of expressing probabilities. Let's calculate the odds ratios and confidence intervals for the coefficients.

```{r message=FALSE}
# print out the coefficients of the model
coef(m)

# compute odds ratios (OR)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
```

Odds ratio can be used to quantify the relationship between our variable and the target variable. Odds higher than 1 mean that the variable is *positively associated* with "success". The odds ratio of our variables can be interpreted as

* **Pstatus**: is connected to intercept and PstatusT as described above. Now the odds ratio for intercept is less than 1 meaning negative association with high alcohol use. On the other hand, PstatusT has positive association (greater than 1). But both have a large 95 % CI in which number 1 is included, so the significance is not strong.
* **Medu**: the variable is very close to 1 (neither positive nor negative association) with 95 % CI including the number 1 so no significance.
* **famrel**: less than 1 with 95 % CI upper end just barely greater than 1. Thus has a somewhat significant negative association with the target variable.
* **absences**: greater than 1 and 95 % CI interval entirely above 1. Positive association with strong significance.

***

**Comparison of the results with our hypotheses**: 

* **Hypothesis 1**: Pstatus=A implies high_use. The results are completely contrary; parents living together has a slightly bigger probability of high alcohol use than parents living apart.
* **Hypothesis 2**: The higher Medu implies no high_use. The results show that while high mother's education does correlate with lower alcohol use, the effect is negligible.
* **Hypothesis 3**: Higher famrel implies no high_use. This hypothesis holds; higher quality of family relations does imply lower alcohol consumption.
* **Hypothesis 4**: Higher absences implies high_use. The hypothesis hold. This is, in fact, the most significant independent variable in the model.


## 3.5 Predictive power of the model

Let's improve our model by discarding the most insignificant variables, ```Pstatus``` and ```Medu```. Now the final model has only variables ```famrel``` and ```absences```.

```{r message=FALSE}
# fix the model
m2 <- glm(high_use ~ famrel+absences, data = alc_test, family ="binomial")
# print out a summary of the model
summary(m2) 
```

It seems that the significance of variables ```famrel``` and ```absences``` has increased slightly compared to the original model. However, the Pr(>|z|) values are almost the same and we have a simpler model than before, so this is an improvement.

Let's use the improved model and **predict** high_use values of an individual.

```{r}
# predict() the probability of high_use
probabilities <- predict(m2, type = "response")
# add the predicted probabilities to 'alc_test'
alc_test <- mutate(alc_test, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc_test <- mutate(alc_test, prediction = probabilities>0.5)
# see the last ten original classes, predicted probabilities, and class predictions
select(alc_test, famrel,absences, high_use, probability, prediction) %>% tail(10)
# tabulate the target variable versus the predictions
select(alc_test, high_use, prediction) %>% table()

library(ggplot2)
# initialize a plot of 'high_use' versus 'probability' in 'alc_test'
g <- ggplot(alc_test, aes(x =probability, y = high_use, col=prediction))

# define the geom as points and draw the plot
g+geom_point()

# tabulate the target variable versus the predictions
table(high_use = alc_test$high_use, prediction = alc_test$prediction) %>% prop.table() %>% addmargins()

# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc_test$high_use, prob = alc_test$probability)
```

To interpret the result, the model ends up predicting wrong 29 % of the time. There is much room for improvement here although it is better than just guessing (50-50 chance). So the model is definitely better than nothing.

## 3.6 10-fold cross-validation of the model

```{r}
# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc_test, cost = loss_func, glmfit = m2, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```

The prediction error obtained here is larger than in the model introduced in the chapter's DataCamp session (prediction error of 0.26). Choosing more significant variables in the model would improve the predictions.
